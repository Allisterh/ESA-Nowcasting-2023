---
title: Methodology
---

We use five different class of models to nowcast the target variables: RegArima, Exponential smoothing, Dynamic factor models, XG boost and Long short-term memory models. Each challenge (production, price, tourism) relies on the same types of models but uses specific datasets and parameters.

## Regarima

### Introduction

ARIMA modelling is a common type of time models used to capture internal information of a series, wether is is stationary or non stationary. ARIMA models offer some advantages in this exercise: they are relatively
simple, easy to interpret, and therefore provide a useful benchmark for more complex models. As standard ARIMA models do not include external information, we use the extended version RegARIMA with external regressors (regression model with ARIMA errors):

$$
y_t = c + \sum_{i=1}^{p}\alpha_i x^i_{t}+u_t
$$
$$
\phi(L)(u_t) = \theta(L)(\varepsilon_t)
$$ 

Additional regressors used in this project are of two types:

- Economic variables presented in the data section. REGARIMA models must keep parsimonious (contrary to other methods implemented in this project), so relevant variables are selected through standard selection
procedures or a priori. 
- Outlier variables such as level-shift or additive outilers to control for atypical observations.

Models are applied to the first differentiation of the variable of interest.

### Automatic identification and nowcasting

We use the RJdemetra package (the TRAMO part) that provide an easy way to identify and estimate RegARIMA models with high flexibility on parameters.

-   we allow for automatic detection of outliers on the estimation period in order to avoid large bias on coefficients. Without adding outliers, "covid" points, for instance, would totally distort coefficients. Outliers identified through this procedure could in addition be used in other methods 
-   computation time is fast (a few seconds for the whole set of countries) 
-   external regressors are selected independently, a priori or through standard variable selection
procedure.

The final nowcast value is provided by the projection of the model at horizon 1, 2 or 3, depending on the challenge and the position in the month.

### Seasonal adjustment for tourism

For tourism, the seasonal component is very strong and may not be the same as the explanatory variables. An X13 pre-treatment is applied to seasonnally adjust and correct for trading days the target variable (Number of nights spent at tourist accommodation establishments) and the potential explanatory variables. This treatment also provides a prediction for the seasonal coefficient of the nowcasted month.

Next, seasonally adjusted variables are put in the REGARIMA model. The final step involves nowcasting the "raw value" by dividing the SA forecast by the projected seasonal coefficient.

## Dynamic Factor Models

Using several sources of data (financial series, macroeconomic series,
surveys...) we estimate a dynamic factor model per country. Missing data
are filled, collinearity is taken into account, data are scaled and log
transformed when necessary. Estimation takes about 20 seconds per
country.

## ETS

Exponential smoothing models: forecasts are linear combinations of past
values, with the weights decaying exponentially as the observations get
older. Estimation takes 1 second by country. No data processing

## XGBoost

### Introduction

XGBoost is a powerful algorithm that has gained popularity in the field
of machine learning due to its ability to handle complex interactions
between variables and its flexibility in handling various types of data.
In the context of Eurostat's nowcasting competition, we utilized the
XGBoost algorithm to predict the values of the Producer Price Index, the
Producer Volume Index and the Number of nights spent at tourist
accommodation establishments for most European countries. We will delve
here into the technicalities of the XGBoost approach, and how we
tailored it to our specific nowcasting problem.

### XGBoost Algorithm

XGBoost is a gradient boosting algorithm that is particularly well
suited for regression and classification problems. It works by building
a sequence of decision trees, each tree trying to correct the errors of
the previous tree. During the training process, the algorithm
iteratively adds decision trees to the model, where each new tree is fit
on the residuals (i.e., the errors) of the previous trees. The final
prediction is made by adding the output of all the trees.

To control for overfitting, XGBoost uses a combination of L1 and L2
regularization, also known as "lasso" and "ridge" regularization,
respectively. These regularization methods add a penalty term to the
loss function, which forces the algorithm to find simpler models. L1
regularization shrinks the less important features' coefficients to
zero, while L2 regularization encourages the coefficients to be small,
but does not set them to zero. By using both methods, XGBoost is able to
produce models that are both accurate and interpretable.

Another key feature of XGBoost is its ability to handle missing values.
Rather than imputing missing values with a fixed value or mean, XGBoost
assigns them a default direction in the split, allowing the algorithm to
learn how to handle missing values during the training process.

Overall, the XGBoost algorithm has proven to be a powerful tool in the
field of machine learning, and its ability to handle large datasets and
complex interactions between variables make it well-suited for
nowcasting problems like the Eurostat competition.

### Transforming Time Series

To apply the XGBoost algorithm to our nowcasting problem, we first
transformed the time series data into a larger dataset tailored for the
algorithm. We gathered several sources of data, including financial
series, macroeconomic series, and surveys, and created a dataset where
each row corresponds to a value per country and per date with many
explanatory variables. We added lagged versions of the target variable
and some of the explanatory variables as additional features. By doing
so, we captured the time series properties of the data and made it
suitable for the XGBoost algorithm.

### Grid Search for Hyperparameters

To obtain optimal results from the XGBoost algorithm, we used a grid
search technique to find the best combination of hyperparameters for
each model. We experimented with various values of hyperparameters,
including learning rate, maximum depth, and subsample ratio, to
determine which combination of parameters resulted in the best
performance. The grid search enabled us to identify the best
hyperparameters for the model, allowing us to obtain the most accurate
predictions. We did not differentiate the hyperparameters for each
country as it would have likely caused even more overfitting.

### Training XGBoost for Nowcasting

To predict our 3 indicators for each country, we trained an XGBoost
model for each country independently. We randomly split the data into
training and testing sets and trained the model on the training set
using the optimal hyperparameters obtained from the grid search. We
evaluated the model's performance on the testing set using various
metrics such as mean squared error and mean absolute error.

## LSTM

LSTM, or Long Short-Term Memory, is a type of recurrent neural network
that is particularly well-suited for modeling time series data. Unlike
traditional feedforward neural networks, which process input data in a
single pass, recurrent neural networks can maintain a "memory" of
previous inputs and use this information to make better predictions.

LSTM takes this a step further by introducing "gates" that can
selectively remember or forget previous inputs based on their importance
to the current prediction. These gates allow the model to capture
longer-term dependencies in the data, making it especially effective for
time series forecasting tasks.

During the training process, the LSTM model is fed a sequence of inputs,
with each input representing a timestep in the time series. The model
then generates a corresponding output for each timestep, which is
compared to the actual value to compute a loss function. The weights of
the model are updated through backpropagation, where the gradient of the
loss function is propagated backwards through the network.

One of the challenges of using LSTMs for time series forecasting is
selecting an appropriate window size, or the number of previous
timesteps that the model should consider when making each prediction. A
larger window size can capture longer-term trends in the data, but may
also introduce more noise and complicate the training process. A smaller
window size, on the other hand, may be more sensitive to short-term
fluctuations in the data, but may not capture longer-term trends as
effectively.

In the context of the Eurostat competition, the LSTM approach was used
to predict the value of several economic indicators, including the
Producer Price Index, the Producer Volume Index and the Number of nights
spent at tourist accommodation establishments. The time series data for
each country was transformed into a format that was suitable for LSTM
training, with each row representing a single timestep in the time
series and the columns representing the various input features.

The LSTM model was trained independently for each country, with a grid
search used to find the optimal hyperparameters for each model. Overall,
the LSTM approach proved to be a powerful tool for time series
forecasting, and its ability to capture long-term dependencies in the
data made it particularly well-suited for the nowcasting problem at
hand.
