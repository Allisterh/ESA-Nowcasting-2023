---
title: Methodology
---

Describe the methodology here.

## Regarima

Automatic ARIMA modelling with detection of outliers and external regressors (financial series, macroeconomic series...). We use the RJdemetra package that provide an easy way to identify and estimate such models with high flexibility on parameters. RegARIMA models offer some advantages in this exercise:

-   they are relatively simple, easy to interpret, and so provide a useful benchmark for more complex models
-   we allow for automatic detection of outliers on the estimation period in order to avoid large bias on coefficients. Without adding outliers, "covid" points would for instance totally distort coefficients. Outliers identified through this procedure could in addition be used in other methods
-   computation time is really fast (a few seconds for the whole set of countries)
-   external regressors can be selected independantly through standard variable selection.

## Dynamic Factor Models

Using several sources of data (financial series, macroeconomic series, surveys...) we estimate a dynamic factor model per country. Missing data are filled, collinearity is taken into account, data are scaled and log transformed when necessary. Estimation takes about 20 seconds per country.

## ETS

Exponential smoothing models: forecasts are linear combinations of past values, with the weights decaying exponentially as the observations get older. Estimation takes 1 second by country. No data processing

## XGBoost

### Introduction

XGBoost is a powerful algorithm that has gained popularity in the field of machine learning due to its ability to handle complex interactions between variables and its flexibility in handling various types of data. In the context of Eurostat's nowcasting competition, we utilized the XGBoost algorithm to predict the values of the Producer Price Index, the Producer Volume Index and the Number of nights spent at tourist accommodation establishments for most European countries. We will delve here into the technicalities of the XGBoost approach, and how we tailored it to our specific nowcasting problem.

### XGBoost Algorithm

XGBoost is a gradient boosting algorithm that is particularly well suited for regression and classification problems. It works by building a sequence of decision trees, each tree trying to correct the errors of the previous tree. During the training process, the algorithm iteratively adds decision trees to the model, where each new tree is fit on the residuals (i.e., the errors) of the previous trees. The final prediction is made by adding the output of all the trees.

To control for overfitting, XGBoost uses a combination of L1 and L2 regularization, also known as "lasso" and "ridge" regularization, respectively. These regularization methods add a penalty term to the loss function, which forces the algorithm to find simpler models. L1 regularization shrinks the less important features' coefficients to zero, while L2 regularization encourages the coefficients to be small, but does not set them to zero. By using both methods, XGBoost is able to produce models that are both accurate and interpretable.

Another key feature of XGBoost is its ability to handle missing values. Rather than imputing missing values with a fixed value or mean, XGBoost assigns them a default direction in the split, allowing the algorithm to learn how to handle missing values during the training process.

Overall, the XGBoost algorithm has proven to be a powerful tool in the field of machine learning, and its ability to handle large datasets and complex interactions between variables make it well-suited for nowcasting problems like the Eurostat competition.

### Transforming Time Series

To apply the XGBoost algorithm to our nowcasting problem, we first transformed the time series data into a larger dataset tailored for the algorithm. We gathered several sources of data, including financial series, macroeconomic series, and surveys, and created a dataset where each row corresponds to a value per country and per date with many explanatory variables. We added lagged versions of the target variable and some of the explanatory variables as additional features. By doing so, we captured the time series properties of the data and made it suitable for the XGBoost algorithm.

### Grid Search for Hyperparameters

To obtain optimal results from the XGBoost algorithm, we used a grid search technique to find the best combination of hyperparameters for each model. We experimented with various values of hyperparameters, including learning rate, maximum depth, and subsample ratio, to determine which combination of parameters resulted in the best performance. The grid search enabled us to identify the best hyperparameters for the model, allowing us to obtain the most accurate predictions. We did not differentiate the hyperparameters for each country as it would have likely caused even more overfitting.

### Training XGBoost for Nowcasting

To predict our 3 indicators for each country, we trained an XGBoost model for each country independently. We randomly split the data into training and testing sets and trained the model on the training set using the optimal hyperparameters obtained from the grid search. We evaluated the model's performance on the testing set using various metrics such as mean squared error and mean absolute error.

## LSTM
